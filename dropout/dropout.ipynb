{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout\n",
    "---\n",
    "Dropout prevents overfitting by randomly shutting down some output units. The video from Coursera vividly illustrate the process.\n",
    "\n",
    "<center>\n",
    "<video width=\"600\" height=\"500\" src=\"images/dropout1_kiank.mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "In the process above, in each iteration, some units on layer `[2]` would be randomly muted, meaning that there would be less neurons working in the forward process, thus the overall structure of neural network is simplified. Meanwhile the trained model would be more robust, since the model no longer can rely on any specific neurons anymore (as they could be muted in the process), all other neurons would need to learn in the training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foward\n",
    "---\n",
    "You can think of dropout as adding an extra layer to the forward process.\n",
    "\n",
    "In the previous sessions, we have the forward equations as following,\n",
    "\n",
    "__Without Dropout__\n",
    "\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} $$\n",
    "\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "\n",
    "Where $g$ is the activation function. Now with dropout an extra layer is applied to $A^{[l]}$.\n",
    "\n",
    "__Dropout__\n",
    "\n",
    "$$ Z^{[l]} = W^{[l]}A^{[l-1]} + b^{[l]} $$\n",
    "\n",
    "$$ A^{[l]} = g^{[l]}(Z^{[l]})$$\n",
    "\n",
    "$$ A^{[l]} = D^{[l]}(A^{[l]})$$\n",
    "\n",
    "Where $D$ is the dropout layer. The key factor in the dropout layer is `keep_prob` parameter, which specifies the probability of keeping each unit. Say if `keep_prob = 0.8`, we would have 80% chance of keeping each output unit as it is, and 20% chance set them to 0.\n",
    "\n",
    "The implementation would be adding an extra mask to the result $A$. Assume we have an output $A^{[l]}$ with four elements as following,\n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "a_1^{[l]} \\\\\n",
    "a_2^{[l]} \\\\\n",
    "a_3^{[l]} \\\\\n",
    "a_4^{[l]}\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "And we want to mute the third unit while keeping the rest, what we need is a matrix of the same shape and do a element-wise multiplication as following,\n",
    "\n",
    "$$ \\begin{pmatrix}\n",
    "a_1^{[l]} \\\\\n",
    "a_2^{[l]} \\\\\n",
    "a_3^{[l]} \\\\\n",
    "a_4^{[l]}\n",
    "\\end{pmatrix} * \n",
    "\\begin{pmatrix}\n",
    "1 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{pmatrix} = \n",
    "\\begin{pmatrix}\n",
    "a_1^{[l]} \\\\\n",
    "a_2^{[l]} \\\\\n",
    "0 \\\\\n",
    "a_4^{[l]}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Let's first initialize some weight parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "layers = [3, 10, 1]\n",
    "def weights_init():\n",
    "    params = {}\n",
    "    n = len(layers)\n",
    "    for i in range(1, n):\n",
    "        params['W' + str(i)] = np.random.randn(layers[i], layers[i-1])*0.01\n",
    "        params['b' + str(i)] = np.zeros((layers[i], 1))\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01273373, -0.0029192 , -0.02510443],\n",
       "        [-0.00323293,  0.01198572, -0.0127173 ],\n",
       "        [ 0.00092662,  0.00766214,  0.02222858],\n",
       "        [-0.01368964, -0.02118227,  0.01315665],\n",
       "        [-0.02065367,  0.01095289,  0.00727299],\n",
       "        [-0.00980028, -0.02437653, -0.0162406 ],\n",
       "        [ 0.00637576, -0.02312436, -0.000291  ],\n",
       "        [-0.0029315 ,  0.01407064,  0.00237895],\n",
       "        [-0.00581215, -0.00695063,  0.00948468],\n",
       "        [-0.00774545, -0.008947  ,  0.01390741]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[ 0.00217584,  0.01116851, -0.01580682,  0.00626901, -0.0053493 ,\n",
       "          0.01537351,  0.00633889,  0.0061288 ,  0.01380906, -0.00308319]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = weights_init()\n",
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_probs = [.5]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "\n",
    "def forward(X):\n",
    "    # intermediate layer use relu as activation\n",
    "    # last layer use sigmoid\n",
    "    n_layers = int(len(params)/2)\n",
    "    A = X\n",
    "    cache = {}\n",
    "    for i in range(1, n_layers):\n",
    "        W, b = params['W'+str(i)], params['b'+str(i)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = relu(Z)\n",
    "        # dropout\n",
    "        keep_prob = keep_probs[i-1]\n",
    "        D = np.random.rand(A.shape[0], A.shape[1])\n",
    "        D = (D < keep_prob).astype(int)\n",
    "        A = np.multiply(D, A)\n",
    "        # rescale\n",
    "        A = A/keep_prob\n",
    "        \n",
    "        cache['Z'+str(i)] = Z\n",
    "        cache['A'+str(i)] = A\n",
    "        cache['D'+str(i)] = D\n",
    "\n",
    "    # last layer\n",
    "    W, b = params['W'+str(i+1)], params['b'+str(i+1)]\n",
    "    Z = np.dot(W, A) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    cache['Z'+str(i+1)] = Z\n",
    "    cache['A'+str(i+1)] = A\n",
    "\n",
    "    return cache, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1.2], [3], [-2]])\n",
    "cache, _ = forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Z1': array([[ 0.05673173],\n",
       "        [ 0.05751222],\n",
       "        [-0.02035879],\n",
       "        [-0.10628767],\n",
       "        [-0.00647173],\n",
       "        [-0.05240875],\n",
       "        [-0.06114016],\n",
       "        [ 0.03393622],\n",
       "        [-0.04679583],\n",
       "        [-0.06395034]]),\n",
       " 'A1': array([[0.        ],\n",
       "        [0.11502445],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ],\n",
       "        [0.        ]]),\n",
       " 'D1': array([[0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [0]]),\n",
       " 'Z2': array([[0.00128465]]),\n",
       " 'A2': array([[0.50032116]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our layers is set to [3, 10, 1], where 3 is the input layer and 1 is the output layer. In the example above we give the hidden layer a `keep_prob` ratio of `0.5`, so some of the units are muted.\n",
    "\n",
    "(__Note__: After dropout, `A` needs to rescale to `A = A / keep_prob`, since some of the units are disabled, the left units need to be amplified in order to match the expected value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward\n",
    "---\n",
    "The backward process is to mask the same function `D` to the corresponding `dA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy code, full version needs to be inside a Class\n",
    "def backward(self, cache, X, Y, keep_probs):\n",
    "    \"\"\"\n",
    "    cache: result [A, Z]\n",
    "    Y: shape (1, m)\n",
    "    \"\"\"\n",
    "    grad = {}\n",
    "    n_layers = int(len(self.params)/2)\n",
    "    m = Y.shape[1]\n",
    "    cache['A0'] = X\n",
    "\n",
    "    for l in range(n_layers, 0, -1):\n",
    "        A, A_prev, Z = cache['A' + str(l)], cache['A' + str(l-1)], cache['Z' + str(l)]\n",
    "        W = self.params['W'+str(l)]\n",
    "        if l == n_layers:\n",
    "            dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "\n",
    "        if l == n_layers:\n",
    "            dZ = np.multiply(dA, self.sigmoid_grad(A, Z))\n",
    "        else:\n",
    "            # dropout version\n",
    "            D = cache['D' + str(l)]\n",
    "            dA = np.multiply(dA, D)\n",
    "            # rescale\n",
    "            dA = dA/keep_probs[l-1]\n",
    "            \n",
    "            dZ = np.multiply(dA, self.relu_grad(A, Z))\n",
    "        dW = np.dot(dZ, A_prev.T)/m\n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA = np.dot(W.T, dZ)\n",
    "\n",
    "        grad['dW'+str(l)] = dW\n",
    "        grad['db'+str(l)] = db\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in back propagation, $dA$ also needs to be rescaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put everything in a class and apply it on a classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class deepNN:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.params = {}\n",
    "        self.dropout = []\n",
    "        self.A = 0\n",
    "        self.Y = 0\n",
    "       \n",
    "    \n",
    "    def weights_init(self):\n",
    "        n = len(self.layers)\n",
    "        for i in range(1, n):\n",
    "            self.params['W' + str(i)] = np.random.randn(self.layers[i], self.layers[i-1])*0.01\n",
    "            self.params['b' + str(i)] = np.zeros((self.layers[i], 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_cost(A, Y):\n",
    "        \"\"\"\n",
    "        For binary classification, both A and Y would have shape (1, m), where m is the batch size\n",
    "        \"\"\"\n",
    "        assert A.shape == Y.shape\n",
    "        m = A.shape[1]\n",
    "        s = np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log((1 - A).T))\n",
    "        loss = -s/m\n",
    "        return np.squeeze(loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_grad(A, Z):\n",
    "        grad = np.multiply(A, 1-A)\n",
    "        return grad\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_grad(A, Z):\n",
    "        grad = np.zeros(Z.shape)\n",
    "        grad[Z>0] = 1\n",
    "        return grad\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        # intermediate layer use relu as activation\n",
    "        # last layer use sigmoid\n",
    "        n_layers = int(len(self.params)/2)\n",
    "        A = X\n",
    "        cache = {}\n",
    "        for i in range(1, n_layers):\n",
    "            W, b = self.params['W'+str(i)], self.params['b'+str(i)]\n",
    "            Z = np.dot(W, A) + b\n",
    "            A = self.relu(Z)\n",
    "            \n",
    "            keep_prob = self.dropout[i-1]\n",
    "            D = np.random.rand(A.shape[0], A.shape[1])\n",
    "            D = np.int64(D < keep_prob)\n",
    "            A = np.multiply(A, D)\n",
    "            A = A/keep_prob\n",
    "            \n",
    "            cache['Z'+str(i)] = Z\n",
    "            cache['A'+str(i)] = A\n",
    "            cache['D'+str(i)] = D\n",
    "\n",
    "        # last layer\n",
    "        W, b = self.params['W'+str(i+1)], self.params['b'+str(i+1)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = self.sigmoid(Z)\n",
    "        cache['Z'+str(i+1)] = Z\n",
    "        cache['A'+str(i+1)] = A\n",
    "\n",
    "        return cache, A\n",
    "    \n",
    "    def backward(self, cache, X, Y):\n",
    "        \"\"\"\n",
    "        cache: result [A, Z]\n",
    "        Y: shape (1, m)\n",
    "        \"\"\"\n",
    "        grad = {}\n",
    "        n_layers = int(len(self.params)/2)\n",
    "        m = Y.shape[1]\n",
    "        cache['A0'] = X\n",
    "\n",
    "        for l in range(n_layers, 0, -1):\n",
    "            A, A_prev, Z = cache['A' + str(l)], cache['A' + str(l-1)], cache['Z' + str(l)]\n",
    "            W = self.params['W'+str(l)]\n",
    "            if l == n_layers:\n",
    "                dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "\n",
    "            if l == n_layers:\n",
    "                dZ = np.multiply(dA, self.sigmoid_grad(A, Z))\n",
    "            else:\n",
    "                keep_prob = self.dropout[l-1]\n",
    "                D = cache['D' + str(l)]\n",
    "                dA = np.multiply(dA, D)\n",
    "                dA = dA/keep_prob\n",
    "                dZ = np.multiply(dA, self.relu_grad(A, Z))\n",
    "            dW = np.dot(dZ, A_prev.T)/m\n",
    "            db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "            dA = np.dot(W.T, dZ)\n",
    "\n",
    "            grad['dW'+str(l)] = dW\n",
    "            grad['db'+str(l)] = db\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def optimize(self, grads, lr):\n",
    "        n_layers = int(len(self.params)/2)\n",
    "        for i in range(1, n_layers+1):\n",
    "            dW, db = grads['dW'+str(i)], grads['db'+str(i)]\n",
    "            self.params['W'+str(i)] -= lr*dW\n",
    "            self.params['b'+str(i)] -= lr*db\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_batch(X, batch_size):\n",
    "        n = X.shape[0]\n",
    "        batches = [range(i, i+batch_size) for i in range(0, n, batch_size)]\n",
    "        return batches\n",
    "    \n",
    "    def train(self, X_train, y_train, batch_size=200, n_iter=100, lr=0.1, dropout:list=[]):\n",
    "        self.dropout = dropout\n",
    "        # prepare batch training\n",
    "        batches = self.generate_batch(X_train, batch_size)\n",
    "        # init weights\n",
    "        self.weights_init()\n",
    "        for i in range(n_iter):\n",
    "            for batch in batches:\n",
    "                X = X_train[batch, :].T\n",
    "                Y = y_train[batch].reshape(1, -1)\n",
    "                cache, A = self.forward(X)\n",
    "                grads = self.backward(cache, X, Y)\n",
    "                self.optimize(grads, lr)\n",
    "\n",
    "            if i%10 == 0:\n",
    "                loss = self.compute_cost(A, Y)\n",
    "                print(f'iteration {i}: loss {loss}')\n",
    "\n",
    "\n",
    "def accuracy(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_pred: vector of predicted value\n",
    "    \"\"\"\n",
    "    \n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    Y_pred = np.round(Y_pred)\n",
    "    acc = float(np.dot(Y, Y_pred.T) + np.dot(1 - Y, 1 - Y_pred.T))/Y.size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (8000, 200)\n",
      "test shape (2000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=200, random_state=123)\n",
    "\n",
    "X_train, X_test = X[:8000], X[8000:]\n",
    "y_train, y_test = y[:8000], y[8000:]\n",
    "\n",
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [200, 100, 20, 1]\n",
    "dropout_ratio = [.8, .8]\n",
    "\n",
    "model = deepNN(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.6931117424217708\n",
      "iteration 10: loss 0.6929932803338776\n",
      "iteration 20: loss 0.6930042095652803\n",
      "iteration 30: loss 0.6929270740459051\n",
      "iteration 40: loss 0.6928159708817516\n",
      "iteration 50: loss 0.6925835619958787\n",
      "iteration 60: loss 0.6922472029699815\n",
      "iteration 70: loss 0.690539000831825\n",
      "iteration 80: loss 0.6813485269598472\n",
      "iteration 90: loss 0.5140425561651449\n",
      "iteration 100: loss 0.26596827522989325\n",
      "iteration 110: loss 0.21297544130219212\n",
      "iteration 120: loss 0.15886453417841173\n",
      "iteration 130: loss 0.14315587310443184\n",
      "iteration 140: loss 0.08922784883210816\n"
     ]
    }
   ],
   "source": [
    "model.train(X_train, y_train, batch_size=200, n_iter=150, lr=0.02, dropout=dropout_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.936\n"
     ]
    }
   ],
   "source": [
    "_, pred = model.forward(X_test.T)\n",
    "acc = accuracy(y_test.reshape(1, -1), pred)\n",
    "\n",
    "print(f'accuracy {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
