{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization\n",
    "---\n",
    "Regularization helps to prevent model from overfitting by adding an extra penalization term at the end of the loss function.\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small  y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} \\tag{1}$$\n",
    "To:\n",
    "$$J_{regularized} = \\small \\underbrace{-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(}\\small y^{(i)}\\log\\left(a^{[L](i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right) \\large{)} }_\\text{cross-entropy cost} + \\underbrace{\\frac{1}{m} \\frac{\\lambda}{2} \\sum\\limits_l\\sum\\limits_k\\sum\\limits_j W_{k,j}^{[l]2} }_\\text{L2 regularization cost} \\tag{2}$$\n",
    "\n",
    "Where $m$ is the batch size. The shown regularization is called `L2 regularization`, where `L2` applies square to weights, `L1 regularization` applies absolute value, which has the form of $|W|$.\n",
    "\n",
    "The appended extra term would enlarge the loss when either there are too many weights or the weight becomes too large, and the adjustable factor $\\lambda$ emphasis on how much we want to penalize on the weights.\n",
    "\n",
    "_**1. Why penalizing weights would help to prevent overfitting?**_\n",
    "\n",
    "An intuitive understanding would be that in the process of minimizing the new loss function, some of the weights would decrease close to zero so that the corresponding neurons would have very small effect to our results, as if we are training on a smaller neural network with fewer neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward\n",
    "---\n",
    "In the forward process, we need only to change the loss function. let's review the cost function we've built in `deepNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from model import deepNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deepNN([2, 4, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7512649762748712\n"
     ]
    }
   ],
   "source": [
    "A = np.array([[.3, .5, .7]])\n",
    "Y = np.array([[1, 1, 1]])\n",
    "\n",
    "loss = model.compute_cost(A, Y)\n",
    "print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(A, Y, parameters, reg=True, lambd=.2):\n",
    "    \"\"\"\n",
    "    With L2 regularization\n",
    "    parameters: dict with 'W1', 'b1', 'W2', ...\n",
    "    \"\"\"\n",
    "    assert A.shape == Y.shape\n",
    "    n_layer = len(parameters)//2\n",
    "    m = A.shape[1]\n",
    "    s = np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log((1 - A).T))\n",
    "    loss = -s/m\n",
    "    if reg:\n",
    "        p = 0\n",
    "        for i in range(1, n_layer+1):\n",
    "            p += np.sum(np.square(parameters['W'+str(i)]))\n",
    "        loss += (1/m)*(lambd/2)*p\n",
    "    return np.squeeze(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.00224882, -0.00683036],\n",
       "        [-0.0155842 ,  0.00439355],\n",
       "        [ 0.0026745 ,  0.00287223],\n",
       "        [-0.00977243,  0.00515391]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'W2': array([[-0.02002206,  0.00227708,  0.00470624,  0.00502016]]),\n",
       " 'b2': array([[0.]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights_init()\n",
    "model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.7512951351356093\n"
     ]
    }
   ],
   "source": [
    "loss = compute_loss(A, Y, model.params)\n",
    "print(f'loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backward\n",
    "---\n",
    "The backward propagation of `L2 reglularization` is actually straight forward, we only need to add the gradient of the L2 term.\n",
    "\n",
    "$$ \\underbrace{\\frac{\\partial{J}^{\\text{L2 Reg}}}{\\partial{W}}}_{\\text{new gradient}} = \\underbrace{ \\frac{\\partial{J}^{\\text{old}}}{\\partial{W}} }_{\\text{new gradient}} + \\frac{\\lambda}{m}|W|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(params, cache, X, Y, lambd=0.2):\n",
    "    \"\"\"\n",
    "    params: weight [W, b]\n",
    "    cache: result [A, Z]\n",
    "    Y: shape (1, m)\n",
    "    \"\"\"\n",
    "    grad = {}\n",
    "    n_layers = int(len(params)/2)\n",
    "    m = Y.shape[1]\n",
    "    cache['A0'] = X\n",
    "    \n",
    "    for l in range(n_layers, 0, -1):\n",
    "        A, A_prev, Z = cache['A' + str(l)], cache['A' + str(l-1)], cache['Z' + str(l)]\n",
    "        W = params['W'+str(l)]\n",
    "        if l == n_layers:\n",
    "            dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "        \n",
    "        if l == n_layers:\n",
    "            dZ = np.multiply(dA, sigmoid_grad(A, Z))\n",
    "        else:\n",
    "            dZ = np.multiply(dA, relu_grad(A, Z))\n",
    "        \n",
    "        # with an extra gradient at the end, other terms would remain the same\n",
    "        dW = np.dot(dZ, A_prev.T)/m + (lambd/m)*W\n",
    "        \n",
    "        db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "        dA = np.dot(W.T, dZ)\n",
    "\n",
    "        grad['dW'+str(l)] = dW\n",
    "        grad['db'+str(l)] = db\n",
    "    \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class deepNN:\n",
    "    def __init__(self, layers):\n",
    "        self.layers = layers\n",
    "        self.params = {}\n",
    "        self.reg = False\n",
    "        self.lambd = .2\n",
    "       \n",
    "    \n",
    "    def weights_init(self):\n",
    "        n = len(self.layers)\n",
    "        for i in range(1, n):\n",
    "            self.params['W' + str(i)] = np.random.randn(self.layers[i], self.layers[i-1])*0.01\n",
    "            self.params['b' + str(i)] = np.zeros((self.layers[i], 1))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1/(1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def relu(x):\n",
    "        return np.maximum(x, 0)\n",
    "    \n",
    "    def compute_loss(self, A, Y):\n",
    "        \"\"\"\n",
    "        With L2 regularization\n",
    "        \"\"\"\n",
    "        assert A.shape == Y.shape\n",
    "        n_layer = len(self.params)//2\n",
    "        m = A.shape[1]\n",
    "        s = np.dot(Y, np.log(A.T)) + np.dot(1-Y, np.log((1 - A).T))\n",
    "        loss = -s/m\n",
    "        if self.reg:\n",
    "            p = 0\n",
    "            for i in range(1, n_layer+1):\n",
    "                p += np.sum(np.square(self.params['W'+str(i)]))\n",
    "            loss += (1/m)*(self.lambd/2)*p\n",
    "        return np.squeeze(loss)\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_grad(A, Z):\n",
    "        grad = np.multiply(A, 1-A)\n",
    "        return grad\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_grad(A, Z):\n",
    "        grad = np.zeros(Z.shape)\n",
    "        grad[Z>0] = 1\n",
    "        return grad\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        # intermediate layer use relu as activation\n",
    "        # last layer use sigmoid\n",
    "        n_layers = int(len(self.params)/2)\n",
    "        A = X\n",
    "        cache = {}\n",
    "        for i in range(1, n_layers):\n",
    "            W, b = self.params['W'+str(i)], self.params['b'+str(i)]\n",
    "            Z = np.dot(W, A) + b\n",
    "            A = self.relu(Z)\n",
    "            cache['Z'+str(i)] = Z\n",
    "            cache['A'+str(i)] = A\n",
    "\n",
    "        # last layer\n",
    "        W, b = self.params['W'+str(i+1)], self.params['b'+str(i+1)]\n",
    "        Z = np.dot(W, A) + b\n",
    "        A = self.sigmoid(Z)\n",
    "        cache['Z'+str(i+1)] = Z\n",
    "        cache['A'+str(i+1)] = A\n",
    "\n",
    "        return cache, A\n",
    "    \n",
    "    def backward(self, cache, X, Y):\n",
    "        \"\"\"\n",
    "        cache: result [A, Z]\n",
    "        Y: shape (1, m)\n",
    "        \"\"\"\n",
    "        grad = {}\n",
    "        n_layers = int(len(self.params)/2)\n",
    "        m = Y.shape[1]\n",
    "        cache['A0'] = X\n",
    "\n",
    "        for l in range(n_layers, 0, -1):\n",
    "            A, A_prev, Z = cache['A' + str(l)], cache['A' + str(l-1)], cache['Z' + str(l)]\n",
    "            W = self.params['W'+str(l)]\n",
    "            if l == n_layers:\n",
    "                dA = -np.divide(Y, A) + np.divide(1 - Y, 1 - A)\n",
    "\n",
    "            if l == n_layers:\n",
    "                dZ = np.multiply(dA, self.sigmoid_grad(A, Z))\n",
    "            else:\n",
    "                dZ = np.multiply(dA, self.relu_grad(A, Z))\n",
    "                \n",
    "            dW = np.dot(dZ, A_prev.T)/m + (self.lambd/m)*W\n",
    "            db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "            dA = np.dot(W.T, dZ)\n",
    "\n",
    "            grad['dW'+str(l)] = dW\n",
    "            grad['db'+str(l)] = db\n",
    "\n",
    "        return grad\n",
    "    \n",
    "    def optimize(self, grads, lr):\n",
    "        n_layers = int(len(self.params)/2)\n",
    "        for i in range(1, n_layers+1):\n",
    "            dW, db = grads['dW'+str(i)], grads['db'+str(i)]\n",
    "            self.params['W'+str(i)] -= lr*dW\n",
    "            self.params['b'+str(i)] -= lr*db\n",
    "    \n",
    "    @staticmethod\n",
    "    def generate_batch(X, batch_size):\n",
    "        n = X.shape[0]\n",
    "        batches = [range(i, i+batch_size) for i in range(0, n, batch_size)]\n",
    "        return batches\n",
    "    \n",
    "    \n",
    "    def train(self, X_train, y_train, batch_size=200, n_iter=100, lr=0.1, reg=True, lambd=.7):\n",
    "        self.lambd = lambd\n",
    "        self.reg = reg\n",
    "        # prepare batch training\n",
    "        batches = self.generate_batch(X_train, batch_size)\n",
    "        # init weights\n",
    "        self.weights_init()\n",
    "        for i in range(n_iter):\n",
    "            for batch in batches:\n",
    "                X = X_train[batch, :].T\n",
    "                Y = y_train[batch].reshape(1, -1)\n",
    "                cache, A = self.forward(X)\n",
    "                grads = self.backward(cache, X, Y)\n",
    "                self.optimize(grads, lr)\n",
    "\n",
    "            if i%10 == 0:\n",
    "                loss = self.compute_loss(A, Y)\n",
    "                print(f'iteration {i}: loss {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_pred: vector of predicted value\n",
    "    \"\"\"\n",
    "    \n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    Y_pred = np.round(Y_pred)\n",
    "    acc = float(np.dot(Y, Y_pred.T) + np.dot(1 - Y, 1 - Y_pred.T))/Y.size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (8000, 200)\n",
      "test shape (2000, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=10000, n_features=200, random_state=123)\n",
    "\n",
    "X_train, X_test = X[:8000], X[8000:]\n",
    "y_train, y_test = y[:8000], y[8000:]\n",
    "\n",
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [200, 100, 20, 1]\n",
    "model = deepNN(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.6985036506635248\n",
      "iteration 10: loss 0.6974289293126693\n",
      "iteration 20: loss 0.696563499398262\n",
      "iteration 30: loss 0.6955727021117409\n",
      "iteration 40: loss 0.6845170595754049\n",
      "iteration 50: loss 0.23561800014771372\n",
      "iteration 60: loss 0.15567224031891935\n",
      "iteration 70: loss 0.12669228589646375\n",
      "iteration 80: loss 0.11069865608869393\n",
      "iteration 90: loss 0.1007637548980789\n",
      "iteration 100: loss 0.09435682482867866\n",
      "iteration 110: loss 0.09060941295356366\n",
      "iteration 120: loss 0.08884491050012915\n",
      "iteration 130: loss 0.08739359237666255\n",
      "iteration 140: loss 0.08695416115831198\n"
     ]
    }
   ],
   "source": [
    "model.train(X_train, y_train, batch_size=200, n_iter=150, lr=0.05, reg=True, lambd=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9425\n"
     ]
    }
   ],
   "source": [
    "_, pred = model.forward(X_test.T)\n",
    "acc = accuracy(y_test.reshape(1, -1), pred)\n",
    "\n",
    "print(f'accuracy {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import deepNN as deepNNOld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [200, 100, 20, 1]\n",
    "model_unreg = deepNNOld(layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually when we have the `iteration` goes up, the model would continue to overfit that causes error in the divide operation, suspecting that in the forward process, result $A$ gets too close to 0.\n",
    "\n",
    "In contrast, the model above with regularization would not overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.6930918829042935\n",
      "iteration 10: loss 0.6930065395149767\n",
      "iteration 20: loss 0.6929575889989992\n",
      "iteration 30: loss 0.6926539088979596\n",
      "iteration 40: loss 0.6849650117201506\n",
      "iteration 50: loss 0.20267451014178056\n",
      "iteration 60: loss 0.09037465413243737\n",
      "iteration 70: loss 0.04981389115902148\n",
      "iteration 80: loss 0.02654689714177362\n",
      "iteration 90: loss 0.015971046473694038\n",
      "iteration 100: loss 0.010199685977249701\n",
      "iteration 110: loss 0.007221608028851772\n",
      "iteration 120: loss 0.004961759731198219\n",
      "iteration 130: loss 0.0034589244309720397\n",
      "iteration 140: loss 0.0025630729230403\n"
     ]
    }
   ],
   "source": [
    "model_unreg.train(X_train, y_train, batch_size=200, n_iter=150, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.9335\n"
     ]
    }
   ],
   "source": [
    "_, pred = model_unreg.forward(X_test.T)\n",
    "acc = accuracy(y_test.reshape(1, -1), pred)\n",
    "\n",
    "print(f'accuracy {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
