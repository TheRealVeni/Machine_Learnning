{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hidden Layer NN\n",
    "---\n",
    "We will build a shallow dense neural network with one hidden layer, and the following structure is used for illustration purpose.\n",
    "\n",
    "<img src='images/1-hidden-nn.png'>\n",
    "\n",
    "Where in the graph above, we have a input vector $x = (x_1, x_2)$, containing 2 features and 4 hidden units $a1, a2, a3$ and $a4$, and output one value $y_1 \\in [0, 1]$ (consider this a binary classification task with a prediction of probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each hidden unit, take $a_1$ as example, a linear operation followed by an activation function is conducted. So given input $x = (x_1, x_2)$, inside node $a_1$, we have:\n",
    "\n",
    "$$z_1 = w_{11}x_1 + w_{12}x_2$$\n",
    "$$a_1 = activation(z_1)$$\n",
    "\n",
    "Here $w_{11}$ denotes weight 1 of node 1, $w_{12}$ denotes weight 2 of node 1. Same for node $a_2$, it would have:\n",
    "\n",
    "$$z_2 = w_{21}x_1 + w_{22}x_2$$\n",
    "$$a_2 = activation(z_2)$$\n",
    "\n",
    "And same for $a_3$ and $a_4$ and so on ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorization of One Input\n",
    "---\n",
    "Now let's put the weights into matrix and input into a vector to simplify the expression.\n",
    "\n",
    "$$ z^{[1]} = W^{[1]}x + b^{[1]} \\tag1 $$\n",
    "\n",
    "$$ a^{[1]} = \\tanh{Z^{[1]}} \\tag2 $$\n",
    "\n",
    "$$ z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\tag3 $$\n",
    "\n",
    "$$ \\hat{y} = a^{[2]} = \\sigma({Z^{[2]}}) \\tag4 $$\n",
    "\n",
    "$$ L(y, \\hat{y}) = -[y\\log{\\hat{y}} + (1 - y)\\log{(1 - \\hat{y})}] $$\n",
    "\n",
    "Here we've assumed that the second activation function to be $\\tanh$ and the output activation function to be $sigmoid$ (note that superscript $[i]$ denotes the $ith$ layer). \n",
    "\n",
    "For the dimension of each matrix, we have:\n",
    "\n",
    "- $ W^{[1]}$ in the case above would have dimension $4 \\times 2$, with each $ith$ row is the weight of node $i$\n",
    "- $b^{[1]}$ has dimension $4 \\times 1$\n",
    "- $z^{[1]}$ and $a^{[1]}$ both have dimention $4 \\times 1$\n",
    "- $W^{[2]}$ has dimension $1 \\times 4$\n",
    "- consequently, $z^{[2]}$ and $a^{[2]}$ would have dimensition $1 \\times 1$, which is a single value\n",
    "\n",
    "The loss function $L$ for a single value would be the same as logistic regression's.\n",
    "\n",
    "Function $\\tanh$ and $sigmoid$ looks as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'tanh')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10, 4])\n",
    "x = np.linspace(-10, 10)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x, sigmoid(x))\n",
    "plt.title('sigmoid')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x, tanh(x))\n",
    "plt.title('tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the only difference of these functions is the scale of y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formula of Batch Training\n",
    "---\n",
    "The above shows the formula of a single input vector, however in actual training processes, a batch is trained instead of 1 at a time. The change applied in the formula is trivial, we just need to replace the single vector $x$ with a matrix $X$ with size $n \\times m$, where $n$ is number of features and $m$ is the the batch size -- samples are stacked column wise, and the following result matrix are applied likewise.\n",
    "\n",
    "$$ Z^{[1]} = W^{[1]}X + b^{[1]} \\tag5 $$\n",
    "\n",
    "$$ A^{[1]} = \\tanh{Z^{[1]}} \\tag6 $$\n",
    "\n",
    "$$ Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\tag7 $$\n",
    "\n",
    "$$ \\hat{Y} = A^{[2]} = \\sigma({Z^{[2]}}) \\tag8 $$\n",
    "\n",
    "$$ J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \\frac{1}{m} \\sum_{i}^{m}L(y^{(i)}, \\hat{y}^{(i)}) \\tag9 $$\n",
    "\n",
    "For the dimension of each matrix taken in this example, we have:\n",
    "\n",
    "- $X$ has dimension $2 \\times m$, as here there are 2 features and $m$ is the batch size\n",
    "- $W^{[1]}$ in the case above would have dimension $4 \\times 2$, with each $ith$ row is the weight of node $i$\n",
    "- $b^{[1]}$ has dimension $4 \\times 1$\n",
    "- $Z^{[1]}$ and $A^{[1]}$ both have dimension $4 \\times m$\n",
    "- $W^{[2]}$ has dimension $1 \\times 4$\n",
    "- consequently, $Z^{[2]}$ and $A^{[2]}$ would have dimension $1 \\times m$\n",
    "\n",
    "Same as logistic regression, for batch training, the average loss for all training samples.\n",
    "\n",
    "This is all for the forward propagation. To activate our neurons to learn, we need to get derivative of weight parameters and update them use gradient descent.\n",
    "\n",
    "But now it is enough for us to implement the forward propagation first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Sample Dataset\n",
    "---\n",
    "Here we generate a simple binary classification task with 5000 data points and 20 features for later model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape (4000, 20)\n",
      "test shape (1000, 20)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "X, y = datasets.make_classification(n_samples=5000, random_state=123)\n",
    "\n",
    "X_train, X_test = X[:4000], X[4000:]\n",
    "y_train, y_test = y[:4000], y[4000:]\n",
    "\n",
    "print('train shape', X_train.shape)\n",
    "print('test shape', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weights Initialization\n",
    "---\n",
    "Our neural network has 1 hidden layer and 2 layers in total(hidden layer + output layer), so there are 4 weight matrices to initialize ($W^{[1]}, b^{[1]}$ and $W^{[2]}, b^{[2]}$). Notice that the weights are initialized relatively small so that the gradients would be higher thus learning faster in the beginning phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(n_input, n_hidden, n_output):\n",
    "    params = {}\n",
    "    params['W1'] = np.random.randn(n_hidden, n_input) * 0.01\n",
    "    params['b1'] = np.zeros((n_hidden, 1))\n",
    "    params['W2'] = np.random.randn(n_output, n_hidden) * 0.01\n",
    "    params['b2'] = np.zeros((n_output, 1))\n",
    "    \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 shape (10, 20)\n",
      "b1 shape (10, 1)\n",
      "W2 shape (1, 10)\n",
      "b2 shape (1, 1)\n"
     ]
    }
   ],
   "source": [
    "params = init_weights(20, 10, 1)\n",
    "\n",
    "print('W1 shape', params['W1'].shape)\n",
    "print('b1 shape', params['b1'].shape)\n",
    "print('W2 shape', params['W2'].shape)\n",
    "print('b2 shape', params['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "---\n",
    "Let's implement the forward process following equations $(5) \\sim (8)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, params):\n",
    "    \"\"\"\n",
    "    X: need to have shape (n_features x m_samples)\n",
    "    \"\"\"\n",
    "    W1, b1, W2, b2 = params['W1'], params['b1'], params['W2'], params['b2']\n",
    "    A0 = X\n",
    "    \n",
    "    cache = {}\n",
    "    Z1 = np.dot(W1, A0) + b1\n",
    "    A1 = tanh(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    cache['Z1'] = Z1\n",
    "    cache['A1'] = A1\n",
    "    cache['Z2'] = Z2\n",
    "    cache['A2'] = A2\n",
    "    return  cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z1 shape (10, 100)\n",
      "A1 shape (10, 100)\n",
      "Z2 shape (1, 100)\n",
      "A2 shape (1, 100)\n"
     ]
    }
   ],
   "source": [
    "# get 100 samples\n",
    "inp = X[:100].T\n",
    "\n",
    "cache = forward(inp, params)\n",
    "\n",
    "print('Z1 shape', cache['Z1'].shape)\n",
    "print('A1 shape', cache['A1'].shape)\n",
    "print('Z2 shape', cache['Z2'].shape)\n",
    "print('A2 shape', cache['A2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "---\n",
    "Following equation $(9)$, let's calculate the loss of each batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(Y, Y_hat):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_hat: vector of predicted value\n",
    "    \"\"\"\n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape == Y_hat.shape\n",
    "    m = Y.shape[1]\n",
    "    s = Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)\n",
    "    loss = -np.sum(s) / m\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.4237153959578408\n"
     ]
    }
   ],
   "source": [
    "Y = np.array([np.random.choice([0, 1]) for i in range(10)]).reshape(1, -1)\n",
    "Y_hat = np.random.uniform(0, 1, 10).reshape(1, -1)\n",
    "\n",
    "l = loss(Y, Y_hat)\n",
    "print(f'loss {l}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back Propagation\n",
    "---\n",
    "Now it comes to the back propagation which is the key to our weights update. Given the loss function $L$ we defined above, we have gradients as follows:\n",
    "\n",
    "$$ dZ^{[2]} = A^{[2]} - Y \\tag1 $$\n",
    "\n",
    "$$ dW^{[2]} = \\frac{1}{m}dZ^{[2]}A^{[1]^T} \\tag2 $$\n",
    "\n",
    "$$ db^{[2]} = \\frac{1}{m}np.sum(dZ^{[2]}, axis=1, keepdims=True) \\tag3 $$\n",
    "\n",
    "$$ dZ^{[1]} = W^{[2]T}dZ^{[2]} * (1 - Z^{[1]^2}) \\tag4 $$\n",
    "\n",
    "$$ dW^{[1]} = \\frac{1}{m}dZ^{[1]}X^{T} \\tag5 $$\n",
    "\n",
    "$$ db^{[1]} = \\frac{1}{m}np.sum(dZ^{[1]}, axis=1, keepdims=True) \\tag6 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In equation $(4)$ is element-wise multiplication, and the gradient of $\\tanh{x}$ is $1 - x^2$. You can try to deduct the equation above by yourself, but I basically took it from internet.\n",
    "\n",
    "Let's break down the shape of each element, given number of each layer equals `(n_x, n_h, n_y)` and batch size equals `m`:\n",
    "\n",
    "- $A^{[2]}$, $Y$ and $dZ^{[2]}$ has shape `(n_y, m)`\n",
    "- Because $A^{[1]}$ has shape `(n_h, m)`, $dW^{[2]}$ would have shape `(n_y, n_h)`\n",
    "- $db^{[2]}$ has shape `(n_y, 1)`\n",
    "\n",
    "- Because $dZ^{[2]}$ has shape `(n_y, m)`, $W^{[2]}$ has shape`(n_y, n_h)`, $dZ^{[1]}$ would have shape `(n_h, m)`\n",
    "- In equation $(5)$, $X$ has shape `(n_x, m)`, so $dW^{[1]}$ has shape `(n_h, n_x)`\n",
    "- $db^{[1]}$ has shape `(n_h, 1)`\n",
    "\n",
    "\n",
    "Once we understand the formula, implementation should come with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(params, cache, X, Y):\n",
    "    \"\"\"\n",
    "    [From coursera deep-learning course]\n",
    "    params: we initiate above with W1, b1, W2, b2\n",
    "    cache: the intermediate caculation we saved with Z1, A1, Z2, A2\n",
    "    X: shape of (n_x, m)\n",
    "    Y: shape (n_y, m)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "\n",
    "    W1 = params['W1']\n",
    "    W2 = params['W2']\n",
    "    A1 = cache['A1']\n",
    "    A2 = cache['A2']\n",
    "\n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Loader\n",
    "---\n",
    "Now let's ensemble everything into a class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNN:\n",
    "    def __init__(self, n_input, n_hidden, n_output):\n",
    "        self.n_input = n_input\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_output = n_output\n",
    "        self.params = {}\n",
    "        self.cache = {}\n",
    "        self.grads = {}\n",
    "        \n",
    "    def compute_loss(self, Y, Y_hat):\n",
    "        \"\"\"\n",
    "        Y: vector of true value\n",
    "        Y_hat: vector of predicted value\n",
    "        \"\"\"\n",
    "        assert Y.shape[0] == 1\n",
    "        assert Y.shape == Y_hat.shape\n",
    "        m = Y.shape[1]\n",
    "        s = Y * np.log(Y_hat) + (1 - Y) * np.log(1 - Y_hat)\n",
    "        loss = -np.sum(s) / m\n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.params['W1'] = np.random.randn(self.n_hidden, self.n_input) * 0.01\n",
    "        self.params['b1'] = np.zeros((self.n_hidden, 1))\n",
    "        self.params['W2'] = np.random.randn(self.n_output, self.n_hidden) * 0.01\n",
    "        self.params['b2'] = np.zeros((self.n_output, 1))\n",
    "    \n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X: need to have shape (n_features x m_samples)\n",
    "        \"\"\"\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        A0 = X\n",
    "\n",
    "        Z1 = np.dot(W1, A0) + b1\n",
    "        A1 = tanh(Z1)\n",
    "        Z2 = np.dot(W2, A1) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        self.cache['Z1'] = Z1\n",
    "        self.cache['A1'] = A1\n",
    "        self.cache['Z2'] = Z2\n",
    "        self.cache['A2'] = A2\n",
    "     \n",
    "    \n",
    "    def backward(self, X, Y):\n",
    "        \"\"\"\n",
    "        [From coursera deep-learning course]\n",
    "        params: we initiate above with W1, b1, W2, b2\n",
    "        cache: the intermediate caculation we saved with Z1, A1, Z2, A2\n",
    "        X: shape of (n_x, m)\n",
    "        Y: shape (n_y, m)\n",
    "        \"\"\"\n",
    "\n",
    "        m = X.shape[1]\n",
    "\n",
    "        W1 = self.params['W1']\n",
    "        W2 = self.params['W2']\n",
    "        A1 = self.cache['A1']\n",
    "        A2 = self.cache['A2']\n",
    "\n",
    "        dZ2 = A2 - Y\n",
    "        dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "        db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))\n",
    "        dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "        db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        self.grads = {\"dW1\": dW1,\n",
    "                      \"db1\": db1,\n",
    "                      \"dW2\": dW2,\n",
    "                      \"db2\": db2}\n",
    "\n",
    "        \n",
    "    def get_batch_indices(self, X_train, batch_size):\n",
    "        n = X_train.shape[0]\n",
    "        indices = [range(i, i+batch_size) for i in range(0, n, batch_size)]\n",
    "        return indices\n",
    "    \n",
    "    \n",
    "    def update_weights(self, lr):\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        dW1, db1, dW2, db2 = self.grads['dW1'], self.grads['db1'], self.grads['dW2'], self.grads['db2']\n",
    "        self.params['W1'] -= dW1\n",
    "        self.params['W2'] -= dW2\n",
    "        self.params['b1'] -= db1\n",
    "        self.params['b2'] -= db2\n",
    "    \n",
    "    \n",
    "    def fit(self, X_train, y_train, batch_size=32, n_iterations=100, lr=0.01):\n",
    "        self.init_weights()\n",
    "        \n",
    "        indices = self.get_batch_indices(X_train, batch_size)\n",
    "        for i in range(n_iterations):\n",
    "            for ind in indices:\n",
    "                X = X_train[ind, :].T\n",
    "                Y = y_train[ind].reshape(1, batch_size)\n",
    "                \n",
    "                self.forward(X)\n",
    "                self.backward(X, Y)\n",
    "                self.update_weights(lr)\n",
    "            \n",
    "            if i % 10 == 0:\n",
    "                Y_hat = self.cache['A2']\n",
    "                loss = self.compute_loss(Y, Y_hat)\n",
    "                print(f'iteration {i}: loss {loss}')\n",
    "            \n",
    "            \n",
    "    def predict(self, X):\n",
    "        W1, b1, W2, b2 = self.params['W1'], self.params['b1'], self.params['W2'], self.params['b2']\n",
    "        A0 = X\n",
    "\n",
    "        Z1 = np.dot(W1, A0) + b1\n",
    "        A1 = tanh(Z1)\n",
    "        Z2 = np.dot(W2, A1) + b2\n",
    "        A2 = sigmoid(Z2)\n",
    "\n",
    "        return A2\n",
    "\n",
    "    \n",
    "def accuracy(Y, Y_pred):\n",
    "    \"\"\"\n",
    "    Y: vector of true value\n",
    "    Y_pred: vector of predicted value\n",
    "    \"\"\"\n",
    "    def _to_binary(x):\n",
    "        return 1 if x > .5 else 0\n",
    "\n",
    "    assert Y.shape[0] == 1\n",
    "    assert Y.shape == Y_pred.shape\n",
    "    Y_pred = np.vectorize(_to_binary)(Y_pred)\n",
    "    acc = float(np.dot(Y, Y_pred.T) + np.dot(1 - Y, 1 - Y_pred.T))/Y.size\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ShallowNN(20, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: loss 0.19575153437513237\n",
      "iteration 10: loss 0.08698150022188056\n",
      "iteration 20: loss 0.07983220808062544\n",
      "iteration 30: loss 0.07437750278137427\n",
      "iteration 40: loss 0.06677985931984107\n",
      "iteration 50: loss 0.05925402693910988\n",
      "iteration 60: loss 0.054844001823287386\n",
      "iteration 70: loss 0.0523565446212034\n",
      "iteration 80: loss 0.051366335822876\n",
      "iteration 90: loss 0.050606347795966344\n",
      "iteration 100: loss 0.04997955343968667\n",
      "iteration 110: loss 0.04945583968865451\n",
      "iteration 120: loss 0.04878268474552334\n",
      "iteration 130: loss 0.04795982774874325\n",
      "iteration 140: loss 0.047676519502507106\n",
      "iteration 150: loss 0.04813515615412707\n",
      "iteration 160: loss 0.04845653653570918\n",
      "iteration 170: loss 0.04849591579686214\n",
      "iteration 180: loss 0.04844944650406025\n",
      "iteration 190: loss 0.048350205002389776\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, batch_size=100, n_iterations=200, lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 95.1%\n"
     ]
    }
   ],
   "source": [
    "y_preds = model.predict(X_test.T)\n",
    "\n",
    "acc = accuracy(y_test.reshape(1, -1), y_preds)\n",
    "print(f'accuracy: {acc*100}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
