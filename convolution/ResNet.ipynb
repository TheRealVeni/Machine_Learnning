{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network\n",
    "---\n",
    "As a neural network goes deeper and deeper, theoretically, it would perform better at least on the training set, but in practice, the performance does not go better with deeper network. One major reason is the `gradient vanishing` problem, in the backpropagation of a very deep network, the gradients at earlier layers would go to zero quickly that cause the learning process to be unbearably slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet\n",
    "---\n",
    "ResNet makes it possible, thoretically, to build a infinite deep neural network without impairing the model performance, at least not getting worse. The major advanced structure in `Resnet` is called `skip connection`.\n",
    "\n",
    "<img src=\"images/images/skip_connection_kiank.png\" style=\"width:650px;height:200px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left side is a plain network without `skip connection`, the right side is the one with a `skip connection`. \n",
    "\n",
    "`skip connection` allows activation values in earlier layers to be fast-forwarded to latter layers by addition. These has 2 advantages:\n",
    "\n",
    "- In the forward propagation, the latter layer would at least have the performance of earlier layers:\n",
    "\n",
    "$$ a^{[l + 2]} = g(z^{[l+2]} + a^{[l]})$$\n",
    "\n",
    "Where $a^{[l]}$ is fast-forwarded from earlier layer. In the iteration process, as weights go to zero, $z$ would decrease to zero, in the worst scenario, even when $z$ totally lost its effect, activation value $a^{[l+2]}$ would at least have the value of $a^{[l]}$.\n",
    "\n",
    "- In the backpropagation process, the gradient can be directly backpropagated to earlier layers so that there would be fewer multiplications in the backpropagation, thus accelerate the learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity Block\n",
    "---\n",
    "There are 2 sorts of `skip connections` in the convolutional network, the first one is called `identity block`, shown as below:\n",
    "\n",
    "<img src=\"images/images/idblock3_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "\n",
    "In this structure the activation value of the earlier layer `x` is directly copied and added to the later layer before send to the `ReLU` activation.\n",
    "\n",
    "Here're the individual steps.\n",
    "\n",
    "First component of the main path: \n",
    "- The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (1,1). Its padding is \"valid\" and its name should be `conv_name_base + '2a'`. Use 0 as the seed for the random initialization. \n",
    "- The first BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2a'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
    "\n",
    "Second component of main path:\n",
    "- The second CONV2D has $F_2$ filters of shape $(f,f)$ and a stride of (1,1). Its padding is \"same\" and its name should be `conv_name_base + '2b'`. Use 0 as the seed for the random initialization. \n",
    "- The second BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2b'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
    "\n",
    "Third component of main path:\n",
    "- The third CONV2D has $F_3$ filters of shape (1,1) and a stride of (1,1). Its padding is \"valid\" and its name should be `conv_name_base + '2c'`. Use 0 as the seed for the random initialization. \n",
    "- The third BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2c'`. Note that there is no ReLU activation function in this component. \n",
    "\n",
    "Final step: \n",
    "- The shortcut and the input are added together.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identity_block(X, f, nfilters, stage, block):\n",
    "    \"\"\"\n",
    "    X has shape (m, n_H, n_W, n_C)\n",
    "    f filter size of second conv + bn + relu\n",
    "    \"\"\"\n",
    "    assert len(nfilters) == 3\n",
    "    F1, F2, F3 = nfilters\n",
    "    \n",
    "    # note: for identity block, the last layer of input X need to has the same dimension of last filter\n",
    "    # to enable Addition in the end\n",
    "    assert F3 == X.shape[3]\n",
    "    conv_base_name = 'conv_' + str(stage) + block\n",
    "    bn_base_name = 'bn_' + str(stage) + block\n",
    "    \n",
    "    X_shortcut = X\n",
    "    # first\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(1, 1), \n",
    "               padding='valid', kernel_initializer='glorot_uniform',\n",
    "               name=conv_base_name + '2a')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_base_name + '2a')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # second\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), \n",
    "               padding='same', kernel_initializer='glorot_uniform',\n",
    "               name=conv_base_name + '2b')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_base_name + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # third\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), \n",
    "               padding='valid', kernel_initializer='glorot_uniform',\n",
    "               name=conv_base_name + '2c')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_base_name + '2c')(X)  # X shape: (m, n_H, n_W, F3)\n",
    "    \n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape (3, 4, 4, 6)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(3, 4, 4, 6)\n",
    "\n",
    "X_output = identity_block(X, f=2, nfilters=[2, 4, 6], stage='1', block='a')\n",
    "\n",
    "print('output shape', X_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Block\n",
    "---\n",
    "Notice that in the `Identity Block` the shape of input $X$ must equal that of the output of the last convolutional layer so that $X$ and $X_{shortcut}$ could add up. So what if the shape doesn't match up? \n",
    "\n",
    "The `convolutional block` help to solve the issue stated above by adding an extra convolutional layer to the shortcut input.\n",
    "\n",
    "<img src=\"images/images/convblock_kiank.png\" style=\"width:650px;height:150px;\">\n",
    "\n",
    "The CONV2D layer in the shortcut path is used to resize the input $x$ to a different dimension so that the dimensions match up in the final addition. (This plays a similar role as the matrix $W_s$ discussed in lecture.) For example, to reduce the activation dimensions's height and width by a factor of 2, you can use a 1x1 convolution with a stride of 2. The CONV2D layer on the shortcut path does not use any non-linear activation function. __Its main role is to just apply a (learned) linear function that reduces the dimension of the input, so that the dimensions match up for the later addition step.__\n",
    "\n",
    "The details of the convolutional block are as follows. \n",
    "\n",
    "First component of the main path:\n",
    "- The first CONV2D has $F_1$ filters of shape (1,1) and a stride of (s,s). Its padding is \"valid\" and its name should be `conv_name_base + '2a'`. \n",
    "- The first BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2a'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
    "\n",
    "Second component of the main path:\n",
    "- The second CONV2D has $F_2$ filters of (f,f) and a stride of (1,1). Its padding is \"same\" and it's name should be `conv_name_base + '2b'`.\n",
    "- The second BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2b'`.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
    "\n",
    "Third component of the main path:\n",
    "- The third CONV2D has $F_3$ filters of (1,1) and a stride of (1,1). Its padding is \"valid\" and it's name should be `conv_name_base + '2c'`.\n",
    "- The third BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '2c'`. Note that there is no ReLU activation function in this component. \n",
    "\n",
    "Shortcut path:\n",
    "- The CONV2D has $F_3$ filters of shape (1,1) and a stride of (s,s). Its padding is \"valid\" and its name should be `conv_name_base + '1'`.\n",
    "- The BatchNorm is normalizing the channels axis.  Its name should be `bn_name_base + '1'`. \n",
    "\n",
    "Final step: \n",
    "- The shortcut and the main path values are added together.\n",
    "- Then apply the ReLU activation function. This has no name and no hyperparameters. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convolutional_block(X, f, nfilters, s, stage, block):\n",
    "    \"\"\"\n",
    "    X: shape of (m, n_H, n_W, n_C)\n",
    "    f: filter size of the second convolutional layer\n",
    "    s: stride size\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(nfilters) == 3\n",
    "    \n",
    "    conv_base_name = 'conv_' + str(stage) + block\n",
    "    bn_base_name = 'bn_' + str(stage) + block\n",
    "    \n",
    "    F1, F2, F3 = nfilters\n",
    "    X_shortcut = X\n",
    "    \n",
    "    # first\n",
    "    X = Conv2D(filters=F1, kernel_size=(1, 1), strides=(s, s), \n",
    "               padding='valid', name=conv_base_name + '2a', kernel_initializer='glorot_uniform')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_base_name + '2a')(X)\n",
    "    X = Activation('relu')(X)  # (n_H - 1)/s + 1\n",
    "    \n",
    "    # second\n",
    "    X = Conv2D(filters=F2, kernel_size=(f, f), strides=(1, 1), \n",
    "               padding='same', name=conv_base_name + '2b', kernel_initializer='glorot_uniform')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_base_name + '2b')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    # third\n",
    "    X = Conv2D(filters=F3, kernel_size=(1, 1), strides=(1, 1), \n",
    "               padding='valid', name=conv_base_name + '2c', kernel_initializer='glorot_uniform')(X)\n",
    "    X = BatchNormalization(axis=3, name=bn_base_name + '2c')(X)\n",
    "    \n",
    "    # shortcut\n",
    "    X_shortcut = Conv2D(filters=F3, kernel_size=(1, 1), strides=(s, s),\n",
    "               padding='valid', name=conv_base_name + '1', kernel_initializer='glorot_uniform')(X_shortcut)\n",
    "    X_shortcut = BatchNormalization(axis=3, name=bn_base_name + '1')(X_shortcut)\n",
    "    \n",
    "    X = Add()([X, X_shortcut])\n",
    "    X = Activation('relu')(X)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output shape (3, 2, 2, 6)\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(3, 4, 4, 6)\n",
    "\n",
    "X_output = convolutional_block(X, f=2, nfilters=[2, 4, 6], s=2, stage='1', block='a')\n",
    "\n",
    "print('output shape', X_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building ResNet model (50 layers)\n",
    "---\n",
    "You now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. \"ID BLOCK\" in the diagram stands for \"Identity block,\" and \"ID BLOCK x3\" means you should stack 3 identity blocks together.\n",
    "\n",
    "<img src=\"images/images/resnet_kiank.png\" style=\"width:850px;height:150px;\">\n",
    "\n",
    "The details of this ResNet-50 model are:\n",
    "- Zero-padding pads the input with a pad of (3,3)\n",
    "- Stage 1:\n",
    "    - The 2D Convolution has 64 filters of shape (7,7) and uses a stride of (2,2). Its name is \"conv1\".\n",
    "    - BatchNorm is applied to the channels axis of the input.\n",
    "    - Activation is `relu`.\n",
    "    - MaxPooling uses a (3,3) window and a (2,2) stride.\n",
    "- Stage 2:\n",
    "    - The convolutional block uses three set of filters of size [64,64,256], \"f\" is 3, \"s\" is 1 and the block is \"a\".\n",
    "    - The 2 identity blocks use three set of filters of size [64,64,256], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- Stage 3:\n",
    "    - The convolutional block uses three set of filters of size [128,128,512], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 3 identity blocks use three set of filters of size [128,128,512], \"f\" is 3 and the blocks are \"b\", \"c\" and \"d\".\n",
    "- Stage 4:\n",
    "    - The convolutional block uses three set of filters of size [256, 256, 1024], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 5 identity blocks use three set of filters of size [256, 256, 1024], \"f\" is 3 and the blocks are \"b\", \"c\", \"d\", \"e\" and \"f\".\n",
    "- Stage 5:\n",
    "    - The convolutional block uses three set of filters of size [512, 512, 2048], \"f\" is 3, \"s\" is 2 and the block is \"a\".\n",
    "    - The 2 identity blocks use three set of filters of size [256, 256, 2048], \"f\" is 3 and the blocks are \"b\" and \"c\".\n",
    "- The 2D Average Pooling uses a window of shape (2,2) and its name is \"avg_pool\".\n",
    "- The flatten doesn't have any hyperparameters or name.\n",
    "- The Fully Connected (Dense) layer reduces its input to the number of classes using a softmax activation. Its name should be `'fc' + str(classes)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet50(input_shape=(64, 64, 3), classes=6):\n",
    "    \n",
    "    X_input = Input(input_shape)\n",
    "    \n",
    "    X = ZeroPadding2D((3, 3))(X_input)\n",
    "    \n",
    "    # stage 1\n",
    "    X = Conv2D(filters=64, kernel_size=(7, 7), strides=(2, 2), name='conv1', kernel_initializer='glorot_uniform')(X)\n",
    "    X = BatchNormalization(axis=3, name='bn1')(X)\n",
    "    X = Activation('relu')(X)\n",
    "    X = MaxPooling2D((3, 3), strides=(2, 2))(X)\n",
    "    \n",
    "    # stage 2\n",
    "    X = convolutional_block(X, nfilters=[64, 64, 256], f=3, s=1, stage=2, block='a')\n",
    "    X = identity_block(X, nfilters=[64, 64, 256], f=3, stage=2, block='b')\n",
    "    X = identity_block(X, nfilters=[64, 64, 256], f=3, stage=2, block='c')\n",
    "    \n",
    "    # stage 3\n",
    "    X = convolutional_block(X, nfilters=[126, 126, 512], f=3, s=2, stage=3, block='a')\n",
    "    X = identity_block(X, nfilters=[126, 126, 512], f=3, stage=3, block='b')\n",
    "    X = identity_block(X, nfilters=[126, 126, 512], f=3, stage=3, block='c')\n",
    "    X = identity_block(X, nfilters=[126, 126, 512], f=3, stage=3, block='d')\n",
    "    \n",
    "    # stage 4\n",
    "    X = convolutional_block(X, nfilters=[256, 256, 1024], f=3, s=2, stage=4, block='a')\n",
    "    X = identity_block(X, nfilters=[256, 256, 1024], f=3, stage=4, block='b')\n",
    "    X = identity_block(X, nfilters=[256, 256, 1024], f=3, stage=4, block='c')\n",
    "    X = identity_block(X, nfilters=[256, 256, 1024], f=3, stage=4, block='d')\n",
    "    X = identity_block(X, nfilters=[256, 256, 1024], f=3, stage=4, block='e')\n",
    "    X = identity_block(X, nfilters=[256, 256, 1024], f=3, stage=4, block='f')\n",
    "    \n",
    "    # stage 5\n",
    "    X = convolutional_block(X, nfilters=[512, 512, 2048], f=3, s=1, stage=5, block='a')\n",
    "    X = identity_block(X, nfilters=[512, 512, 2048], f=3, stage=5, block='b')\n",
    "    X = identity_block(X, nfilters=[512, 512, 2048], f=3, stage=5, block='c')\n",
    "    \n",
    "    X = AveragePooling2D((2, 2), name='avg_pool')(X)\n",
    "    X = Flatten()(X)\n",
    "    X = Dense(classes, activation='softmax', name='fc' + str(classes), kernel_initializer='glorot_uniform')(X)\n",
    "    \n",
    "    model = Model(X_input, X, name='resnet50')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = resnet50(input_shape=(64, 64, 3), classes=6)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 64, 64, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 70, 70, 3)    0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv2D)                  (None, 32, 32, 64)   9472        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 32, 32, 64)   256         conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 32, 32, 64)   0           bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 15, 15, 64)   0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2a2a (Conv2D)              (None, 15, 15, 64)   4160        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_2a2a (BatchNormalization)    (None, 15, 15, 64)   256         conv_2a2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 15, 15, 64)   0           bn_2a2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2a2b (Conv2D)              (None, 15, 15, 64)   36928       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn_2a2b (BatchNormalization)    (None, 15, 15, 64)   256         conv_2a2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 15, 15, 64)   0           bn_2a2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2a2c (Conv2D)              (None, 15, 15, 256)  16640       activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2a1 (Conv2D)               (None, 15, 15, 256)  16640       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_2a2c (BatchNormalization)    (None, 15, 15, 256)  1024        conv_2a2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_2a1 (BatchNormalization)     (None, 15, 15, 256)  1024        conv_2a1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 15, 15, 256)  0           bn_2a2c[0][0]                    \n",
      "                                                                 bn_2a1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 15, 15, 256)  0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_2b2a (Conv2D)              (None, 15, 15, 64)   16448       activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "bn_2b2a (BatchNormalization)    (None, 15, 15, 64)   256         conv_2b2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 15, 15, 64)   0           bn_2b2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2b2b (Conv2D)              (None, 15, 15, 64)   36928       activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_2b2b (BatchNormalization)    (None, 15, 15, 64)   256         conv_2b2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 15, 15, 64)   0           bn_2b2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2b2c (Conv2D)              (None, 15, 15, 256)  16640       activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_2b2c (BatchNormalization)    (None, 15, 15, 256)  1024        conv_2b2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 15, 15, 256)  0           bn_2b2c[0][0]                    \n",
      "                                                                 activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 15, 15, 256)  0           add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_2c2a (Conv2D)              (None, 15, 15, 64)   16448       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_2c2a (BatchNormalization)    (None, 15, 15, 64)   256         conv_2c2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_13 (Activation)      (None, 15, 15, 64)   0           bn_2c2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2c2b (Conv2D)              (None, 15, 15, 64)   36928       activation_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_2c2b (BatchNormalization)    (None, 15, 15, 64)   256         conv_2c2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_14 (Activation)      (None, 15, 15, 64)   0           bn_2c2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_2c2c (Conv2D)              (None, 15, 15, 256)  16640       activation_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_2c2c (BatchNormalization)    (None, 15, 15, 256)  1024        conv_2c2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 15, 15, 256)  0           bn_2c2c[0][0]                    \n",
      "                                                                 activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_15 (Activation)      (None, 15, 15, 256)  0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_3a2a (Conv2D)              (None, 8, 8, 126)    32382       activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3a2a (BatchNormalization)    (None, 8, 8, 126)    504         conv_3a2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_16 (Activation)      (None, 8, 8, 126)    0           bn_3a2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3a2b (Conv2D)              (None, 8, 8, 126)    143010      activation_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3a2b (BatchNormalization)    (None, 8, 8, 126)    504         conv_3a2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_17 (Activation)      (None, 8, 8, 126)    0           bn_3a2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3a2c (Conv2D)              (None, 8, 8, 512)    65024       activation_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_3a1 (Conv2D)               (None, 8, 8, 512)    131584      activation_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3a2c (BatchNormalization)    (None, 8, 8, 512)    2048        conv_3a2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_3a1 (BatchNormalization)     (None, 8, 8, 512)    2048        conv_3a1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 8, 8, 512)    0           bn_3a2c[0][0]                    \n",
      "                                                                 bn_3a1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_18 (Activation)      (None, 8, 8, 512)    0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_3b2a (Conv2D)              (None, 8, 8, 126)    64638       activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3b2a (BatchNormalization)    (None, 8, 8, 126)    504         conv_3b2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_19 (Activation)      (None, 8, 8, 126)    0           bn_3b2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3b2b (Conv2D)              (None, 8, 8, 126)    143010      activation_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3b2b (BatchNormalization)    (None, 8, 8, 126)    504         conv_3b2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_20 (Activation)      (None, 8, 8, 126)    0           bn_3b2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3b2c (Conv2D)              (None, 8, 8, 512)    65024       activation_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3b2c (BatchNormalization)    (None, 8, 8, 512)    2048        conv_3b2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 8, 8, 512)    0           bn_3b2c[0][0]                    \n",
      "                                                                 activation_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_21 (Activation)      (None, 8, 8, 512)    0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_3c2a (Conv2D)              (None, 8, 8, 126)    64638       activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3c2a (BatchNormalization)    (None, 8, 8, 126)    504         conv_3c2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_22 (Activation)      (None, 8, 8, 126)    0           bn_3c2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3c2b (Conv2D)              (None, 8, 8, 126)    143010      activation_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3c2b (BatchNormalization)    (None, 8, 8, 126)    504         conv_3c2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_23 (Activation)      (None, 8, 8, 126)    0           bn_3c2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3c2c (Conv2D)              (None, 8, 8, 512)    65024       activation_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3c2c (BatchNormalization)    (None, 8, 8, 512)    2048        conv_3c2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 8, 8, 512)    0           bn_3c2c[0][0]                    \n",
      "                                                                 activation_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_24 (Activation)      (None, 8, 8, 512)    0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_3d2a (Conv2D)              (None, 8, 8, 126)    64638       activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3d2a (BatchNormalization)    (None, 8, 8, 126)    504         conv_3d2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_25 (Activation)      (None, 8, 8, 126)    0           bn_3d2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3d2b (Conv2D)              (None, 8, 8, 126)    143010      activation_25[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3d2b (BatchNormalization)    (None, 8, 8, 126)    504         conv_3d2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_26 (Activation)      (None, 8, 8, 126)    0           bn_3d2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_3d2c (Conv2D)              (None, 8, 8, 512)    65024       activation_26[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_3d2c (BatchNormalization)    (None, 8, 8, 512)    2048        conv_3d2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 8, 8, 512)    0           bn_3d2c[0][0]                    \n",
      "                                                                 activation_24[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_27 (Activation)      (None, 8, 8, 512)    0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_4a2a (Conv2D)              (None, 4, 4, 256)    131328      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4a2a (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4a2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_28 (Activation)      (None, 4, 4, 256)    0           bn_4a2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4a2b (Conv2D)              (None, 4, 4, 256)    590080      activation_28[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4a2b (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4a2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_29 (Activation)      (None, 4, 4, 256)    0           bn_4a2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4a2c (Conv2D)              (None, 4, 4, 1024)   263168      activation_29[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_4a1 (Conv2D)               (None, 4, 4, 1024)   525312      activation_27[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4a2c (BatchNormalization)    (None, 4, 4, 1024)   4096        conv_4a2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_4a1 (BatchNormalization)     (None, 4, 4, 1024)   4096        conv_4a1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 4, 4, 1024)   0           bn_4a2c[0][0]                    \n",
      "                                                                 bn_4a1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_30 (Activation)      (None, 4, 4, 1024)   0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv_4b2a (Conv2D)              (None, 4, 4, 256)    262400      activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4b2a (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4b2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_31 (Activation)      (None, 4, 4, 256)    0           bn_4b2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4b2b (Conv2D)              (None, 4, 4, 256)    590080      activation_31[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4b2b (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4b2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_32 (Activation)      (None, 4, 4, 256)    0           bn_4b2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4b2c (Conv2D)              (None, 4, 4, 1024)   263168      activation_32[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4b2c (BatchNormalization)    (None, 4, 4, 1024)   4096        conv_4b2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 4, 4, 1024)   0           bn_4b2c[0][0]                    \n",
      "                                                                 activation_30[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_33 (Activation)      (None, 4, 4, 1024)   0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4c2a (Conv2D)              (None, 4, 4, 256)    262400      activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4c2a (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4c2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_34 (Activation)      (None, 4, 4, 256)    0           bn_4c2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4c2b (Conv2D)              (None, 4, 4, 256)    590080      activation_34[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4c2b (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4c2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_35 (Activation)      (None, 4, 4, 256)    0           bn_4c2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4c2c (Conv2D)              (None, 4, 4, 1024)   263168      activation_35[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4c2c (BatchNormalization)    (None, 4, 4, 1024)   4096        conv_4c2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 4, 4, 1024)   0           bn_4c2c[0][0]                    \n",
      "                                                                 activation_33[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_36 (Activation)      (None, 4, 4, 1024)   0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4d2a (Conv2D)              (None, 4, 4, 256)    262400      activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4d2a (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4d2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_37 (Activation)      (None, 4, 4, 256)    0           bn_4d2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4d2b (Conv2D)              (None, 4, 4, 256)    590080      activation_37[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4d2b (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4d2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_38 (Activation)      (None, 4, 4, 256)    0           bn_4d2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4d2c (Conv2D)              (None, 4, 4, 1024)   263168      activation_38[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4d2c (BatchNormalization)    (None, 4, 4, 1024)   4096        conv_4d2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 4, 4, 1024)   0           bn_4d2c[0][0]                    \n",
      "                                                                 activation_36[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_39 (Activation)      (None, 4, 4, 1024)   0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4e2a (Conv2D)              (None, 4, 4, 256)    262400      activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4e2a (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4e2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_40 (Activation)      (None, 4, 4, 256)    0           bn_4e2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4e2b (Conv2D)              (None, 4, 4, 256)    590080      activation_40[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4e2b (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4e2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_41 (Activation)      (None, 4, 4, 256)    0           bn_4e2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4e2c (Conv2D)              (None, 4, 4, 1024)   263168      activation_41[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4e2c (BatchNormalization)    (None, 4, 4, 1024)   4096        conv_4e2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 4, 4, 1024)   0           bn_4e2c[0][0]                    \n",
      "                                                                 activation_39[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_42 (Activation)      (None, 4, 4, 1024)   0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_4f2a (Conv2D)              (None, 4, 4, 256)    262400      activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4f2a (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4f2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_43 (Activation)      (None, 4, 4, 256)    0           bn_4f2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4f2b (Conv2D)              (None, 4, 4, 256)    590080      activation_43[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4f2b (BatchNormalization)    (None, 4, 4, 256)    1024        conv_4f2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_44 (Activation)      (None, 4, 4, 256)    0           bn_4f2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_4f2c (Conv2D)              (None, 4, 4, 1024)   263168      activation_44[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_4f2c (BatchNormalization)    (None, 4, 4, 1024)   4096        conv_4f2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 4, 4, 1024)   0           bn_4f2c[0][0]                    \n",
      "                                                                 activation_42[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_45 (Activation)      (None, 4, 4, 1024)   0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_5a2a (Conv2D)              (None, 4, 4, 512)    524800      activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_5a2a (BatchNormalization)    (None, 4, 4, 512)    2048        conv_5a2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_46 (Activation)      (None, 4, 4, 512)    0           bn_5a2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_5a2b (Conv2D)              (None, 4, 4, 512)    2359808     activation_46[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_5a2b (BatchNormalization)    (None, 4, 4, 512)    2048        conv_5a2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_47 (Activation)      (None, 4, 4, 512)    0           bn_5a2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_5a2c (Conv2D)              (None, 4, 4, 2048)   1050624     activation_47[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_5a1 (Conv2D)               (None, 4, 4, 2048)   2099200     activation_45[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_5a2c (BatchNormalization)    (None, 4, 4, 2048)   8192        conv_5a2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "bn_5a1 (BatchNormalization)     (None, 4, 4, 2048)   8192        conv_5a1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 4, 4, 2048)   0           bn_5a2c[0][0]                    \n",
      "                                                                 bn_5a1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_48 (Activation)      (None, 4, 4, 2048)   0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_5b2a (Conv2D)              (None, 4, 4, 512)    1049088     activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_5b2a (BatchNormalization)    (None, 4, 4, 512)    2048        conv_5b2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_49 (Activation)      (None, 4, 4, 512)    0           bn_5b2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_5b2b (Conv2D)              (None, 4, 4, 512)    2359808     activation_49[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_5b2b (BatchNormalization)    (None, 4, 4, 512)    2048        conv_5b2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_50 (Activation)      (None, 4, 4, 512)    0           bn_5b2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_5b2c (Conv2D)              (None, 4, 4, 2048)   1050624     activation_50[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_5b2c (BatchNormalization)    (None, 4, 4, 2048)   8192        conv_5b2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 4, 4, 2048)   0           bn_5b2c[0][0]                    \n",
      "                                                                 activation_48[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_51 (Activation)      (None, 4, 4, 2048)   0           add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_5c2a (Conv2D)              (None, 4, 4, 512)    1049088     activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_5c2a (BatchNormalization)    (None, 4, 4, 512)    2048        conv_5c2a[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_52 (Activation)      (None, 4, 4, 512)    0           bn_5c2a[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_5c2b (Conv2D)              (None, 4, 4, 512)    2359808     activation_52[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_5c2b (BatchNormalization)    (None, 4, 4, 512)    2048        conv_5c2b[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_53 (Activation)      (None, 4, 4, 512)    0           bn_5c2b[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_5c2c (Conv2D)              (None, 4, 4, 2048)   1050624     activation_53[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "bn_5c2c (BatchNormalization)    (None, 4, 4, 2048)   8192        conv_5c2c[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 4, 4, 2048)   0           bn_5c2c[0][0]                    \n",
      "                                                                 activation_51[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "activation_54 (Activation)      (None, 4, 4, 2048)   0           add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (AveragePooling2D)     (None, 2, 2, 2048)   0           activation_54[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 8192)         0           avg_pool[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "fc6 (Dense)                     (None, 6)            49158       flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 23,610,822\n",
      "Trainable params: 23,557,734\n",
      "Non-trainable params: 53,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
